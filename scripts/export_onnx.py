"""
Script para exportar el modelo PyTorch entrenado a formato ONNX.
"""
import sys
import torch
import torch.onnx
import onnx
import onnxruntime as ort
import numpy as np
from pathlib import Path
import json

# A√±adir directorio ra√≠z al path
ROOT_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(ROOT_DIR))

from src.cnn_model_v2 import CharCNN_v2

def export_to_onnx():
    print("üöÄ Iniciando exportaci√≥n a ONNX...")
    
    # Rutas
    model_dir = ROOT_DIR / 'models' / 'cnn_modelo_v2_finetuned'
    weights_path = model_dir / 'best_model_finetuned.pth'
    output_path = model_dir / 'model.onnx'
    
    if not weights_path.exists():
        print(f"‚ùå Error: No se encontraron los pesos en {weights_path}")
        return
    
    # 1. Cargar checkpoint para inferir configuraci√≥n
    print("‚öñÔ∏è Cargando checkpoint...")
    device = torch.device('cpu')
    checkpoint = torch.load(weights_path, map_location=device)
    
    state_dict = checkpoint['model_state_dict'] if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint else checkpoint
    
    # Inferir num_classes de la √∫ltima capa (fc4.weight)
    if 'fc4.weight' in state_dict:
        num_classes = state_dict['fc4.weight'].shape[0]
        print(f"üß† Configuraci√≥n inferida: num_classes={num_classes}")
    else:
        print("‚ö†Ô∏è No se pudo inferir num_classes, usando valor por defecto 62")
        num_classes = 62
        
    dropout_rate = 0.5 # Valor seguro por defecto
    
    # 2. Inicializar modelo
    print("üî® Inicializando modelo...")
    model = CharCNN_v2(num_classes=num_classes, dropout_rate=dropout_rate)
    
    # 3. Cargar pesos
    model.load_state_dict(state_dict)
    model.eval()
    
    # 4. Crear input dummy
    dummy_input = torch.randn(1, 1, 28, 28, requires_grad=True)
    
    # 5. Exportar
    print(f"üì¶ Exportando a {output_path}...")
    torch.onnx.export(
        model,
        dummy_input,
        str(output_path),
        export_params=True,
        opset_version=12,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    print("‚úÖ Exportaci√≥n completada.")
    
    # 6. Verificaci√≥n
    verify_onnx(str(output_path), model, dummy_input)

def verify_onnx(onnx_path, torch_model, dummy_input):
    print("\nüîç Verificando modelo ONNX...")
    
    # Verificar estructura
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    print("‚úÖ Estructura del modelo ONNX v√°lida.")
    
    # Comparar salidas
    print("üß™ Comparando inferencia PyTorch vs ONNX Runtime...")
    
    # PyTorch output
    with torch.no_grad():
        torch_out = torch_model(dummy_input)
    
    # ONNX Runtime output
    ort_session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])
    
    def to_numpy(tensor):
        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()
    
    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}
    ort_outs = ort_session.run(None, ort_inputs)
    
    # Comparar
    np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)
    print("‚úÖ ¬°Las salidas coinciden! El modelo exportado es fiel al original.")
    print(f"üéâ Modelo listo en: {onnx_path}")

if __name__ == '__main__':
    try:
        export_to_onnx()
    except Exception as e:
        print(f"\n‚ùå Error fatal durante la exportaci√≥n: {e}")
        sys.exit(1)
